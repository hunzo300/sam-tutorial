{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: image_encoder.neck.0.weight, Shape: torch.Size([256, 768, 1, 1])\n",
      "Parameter: image_encoder.neck.1.weight, Shape: torch.Size([256])\n",
      "Parameter: image_encoder.neck.1.bias, Shape: torch.Size([256])\n",
      "Parameter: image_encoder.neck.2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Parameter: image_encoder.neck.3.weight, Shape: torch.Size([256])\n",
      "Parameter: image_encoder.neck.3.bias, Shape: torch.Size([256])\n",
      "Parameter: image_encoder.patch_embed.proj.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Parameter: image_encoder.patch_embed.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.0.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.0.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.0.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.0.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.0.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.1.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.1.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.1.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.1.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.1.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.2.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.2.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.2.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.2.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.2.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.3.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.3.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.3.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.3.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.3.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.4.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.4.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.4.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.4.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.4.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.5.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.5.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.5.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.5.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.5.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.6.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.6.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.6.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.6.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.6.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.7.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.7.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.7.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.7.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.7.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.8.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.8.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.8.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.8.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.8.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.9.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.9.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.9.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.9.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.9.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.10.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.10.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.10.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.10.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.10.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.11.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.11.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.11.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.11.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.11.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: prompt_encoder.pe_layer.positional_encoding_gaussian_matrix, Shape: torch.Size([2, 128])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.q_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.q_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.k_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.v_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.out_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm1.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm2.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin1.weight, Shape: torch.Size([2048, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin1.bias, Shape: torch.Size([2048])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin2.weight, Shape: torch.Size([256, 2048])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm3.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm3.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm4.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm4.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.q_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.q_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.k_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.v_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.out_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm1.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm2.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin1.weight, Shape: torch.Size([2048, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin1.bias, Shape: torch.Size([2048])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin2.weight, Shape: torch.Size([256, 2048])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm3.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm3.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm4.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm4.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.norm_final_attn.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.norm_final_attn.bias, Shape: torch.Size([256])\n",
      "Parameter: prompt_encoder.point_embeddings.0.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.point_embeddings.1.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.point_embeddings.2.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.point_embeddings.3.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.not_a_point_embed.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: mask_decoder.output_upscaling.0.weight, Shape: torch.Size([256, 64, 2, 2])\n",
      "Parameter: mask_decoder.output_upscaling.0.bias, Shape: torch.Size([64])\n",
      "Parameter: mask_decoder.output_upscaling.1.weight, Shape: torch.Size([64])\n",
      "Parameter: mask_decoder.output_upscaling.1.bias, Shape: torch.Size([64])\n",
      "Parameter: mask_decoder.output_upscaling.3.weight, Shape: torch.Size([64, 32, 2, 2])\n",
      "Parameter: mask_decoder.output_upscaling.3.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: prompt_encoder.mask_downscaling.0.weight, Shape: torch.Size([4, 1, 2, 2])\n",
      "Parameter: prompt_encoder.mask_downscaling.0.bias, Shape: torch.Size([4])\n",
      "Parameter: prompt_encoder.mask_downscaling.1.weight, Shape: torch.Size([4])\n",
      "Parameter: prompt_encoder.mask_downscaling.1.bias, Shape: torch.Size([4])\n",
      "Parameter: prompt_encoder.mask_downscaling.3.weight, Shape: torch.Size([16, 4, 2, 2])\n",
      "Parameter: prompt_encoder.mask_downscaling.3.bias, Shape: torch.Size([16])\n",
      "Parameter: prompt_encoder.mask_downscaling.4.weight, Shape: torch.Size([16])\n",
      "Parameter: prompt_encoder.mask_downscaling.4.bias, Shape: torch.Size([16])\n",
      "Parameter: prompt_encoder.mask_downscaling.6.weight, Shape: torch.Size([256, 16, 1, 1])\n",
      "Parameter: prompt_encoder.mask_downscaling.6.bias, Shape: torch.Size([256])\n",
      "Parameter: prompt_encoder.no_mask_embed.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.2.weight, Shape: torch.Size([4, 256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.2.bias, Shape: torch.Size([4])\n",
      "Parameter: mask_decoder.iou_token.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: mask_decoder.mask_tokens.weight, Shape: torch.Size([4, 256])\n",
      "Parameter: image_encoder.pos_embed, Shape: torch.Size([1, 64, 64, 768])\n",
      "Parameter: image_encoder.neck.0.weight, Value: tensor([[[[-0.0510]],\n",
      "\n",
      "         [[ 0.1294]],\n",
      "\n",
      "         [[ 0.0209]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0340]],\n",
      "\n",
      "         [[ 0.0122]],\n",
      "\n",
      "         [[ 0.1178]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0541]],\n",
      "\n",
      "         [[ 0.1535]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0094]],\n",
      "\n",
      "         [[ 0.0434]],\n",
      "\n",
      "         [[-0.0605]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0095]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         [[-0.0643]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0694]],\n",
      "\n",
      "         [[ 0.0617]],\n",
      "\n",
      "         [[-0.0471]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0151]],\n",
      "\n",
      "         [[-0.0122]],\n",
      "\n",
      "         [[-0.0120]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0715]],\n",
      "\n",
      "         [[ 0.1192]],\n",
      "\n",
      "         [[ 0.0231]]],\n",
      "\n",
      "\n",
      "        [[[-0.0165]],\n",
      "\n",
      "         [[-0.1485]],\n",
      "\n",
      "         [[ 0.0536]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[ 0.0589]],\n",
      "\n",
      "         [[ 0.0174]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0797]],\n",
      "\n",
      "         [[ 0.1000]],\n",
      "\n",
      "         [[-0.1291]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0839]],\n",
      "\n",
      "         [[ 0.0108]],\n",
      "\n",
      "         [[-0.0588]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = '/home/minkyukim/sam-tutorial/workdir/MedSAM/medsam_vit_b.pth'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "for name, param in checkpoint.items():\n",
    "    print(f\"Parameter: {name}, Shape: {param.shape}\")\n",
    "\n",
    "\n",
    "param_name = list(checkpoint.keys())[0]\n",
    "param_value = checkpoint[param_name]\n",
    "print(f\"Parameter: {param_name}, Value: {param_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
