{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_946088/359729426.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: image_encoder.pos_embed, Shape: torch.Size([1, 64, 64, 768])\n",
      "Parameter: image_encoder.patch_embed.proj.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Parameter: image_encoder.patch_embed.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.0.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.0.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.0.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.0.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.0.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.0.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.0.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.0.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.0.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.0.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.1.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.1.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.1.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.1.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.1.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.1.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.1.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.1.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.1.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.1.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.1.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.2.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.2.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.2.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.2.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.2.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.2.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.2.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.2.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.2.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.2.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.2.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.3.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.3.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.3.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.3.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.3.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.3.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.3.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.3.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.3.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.3.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.3.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.4.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.4.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.4.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.4.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.4.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.4.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.4.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.4.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.4.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.4.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.4.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.5.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.5.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.5.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.5.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.5.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.5.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.5.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.5.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.5.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.5.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.5.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.6.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.6.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.6.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.6.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.6.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.6.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.6.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.6.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.6.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.6.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.6.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.7.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.7.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.7.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.7.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.7.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.7.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.7.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.7.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.7.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.7.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.7.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.8.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.8.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.8.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.8.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.8.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.8.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.8.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.8.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.8.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.8.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.8.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.9.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.9.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.9.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.9.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.9.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.9.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.9.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.9.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.9.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.9.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.9.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.10.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.attn.rel_pos_h, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.10.attn.rel_pos_w, Shape: torch.Size([27, 64])\n",
      "Parameter: image_encoder.blocks.10.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.10.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.10.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.10.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.10.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.10.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.10.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.10.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.10.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.blocks.11.norm1.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.norm1.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.attn.rel_pos_h, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.11.attn.rel_pos_w, Shape: torch.Size([127, 64])\n",
      "Parameter: image_encoder.blocks.11.attn.qkv.weight, Shape: torch.Size([2304, 768])\n",
      "Parameter: image_encoder.blocks.11.attn.qkv.bias, Shape: torch.Size([2304])\n",
      "Parameter: image_encoder.blocks.11.attn.proj.weight, Shape: torch.Size([768, 768])\n",
      "Parameter: image_encoder.blocks.11.attn.proj.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.norm2.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.norm2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin1.weight, Shape: torch.Size([3072, 768])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin1.bias, Shape: torch.Size([3072])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin2.weight, Shape: torch.Size([768, 3072])\n",
      "Parameter: image_encoder.blocks.11.mlp.lin2.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.Adapter.norm.weight, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.Adapter.norm.bias, Shape: torch.Size([768])\n",
      "Parameter: image_encoder.blocks.11.Adapter.channel.0.weight, Shape: torch.Size([192, 768])\n",
      "Parameter: image_encoder.blocks.11.Adapter.channel.2.weight, Shape: torch.Size([768, 192])\n",
      "Parameter: image_encoder.blocks.11.Adapter.spatial.0.weight, Shape: torch.Size([768, 768, 3, 3])\n",
      "Parameter: image_encoder.blocks.11.Adapter.spatial.2.weight, Shape: torch.Size([768, 768, 4, 4])\n",
      "Parameter: image_encoder.neck.0.weight, Shape: torch.Size([256, 768, 1, 1])\n",
      "Parameter: image_encoder.neck.1.weight, Shape: torch.Size([256])\n",
      "Parameter: image_encoder.neck.1.bias, Shape: torch.Size([256])\n",
      "Parameter: image_encoder.neck.2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Parameter: image_encoder.neck.3.weight, Shape: torch.Size([256])\n",
      "Parameter: image_encoder.neck.3.bias, Shape: torch.Size([256])\n",
      "Parameter: prompt_encoder.pe_layer.positional_encoding_gaussian_matrix, Shape: torch.Size([2, 128])\n",
      "Parameter: prompt_encoder.point_embeddings.0.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.point_embeddings.1.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.point_embeddings.2.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.point_embeddings.3.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.not_a_point_embed.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: prompt_encoder.mask_downscaling.0.weight, Shape: torch.Size([4, 1, 2, 2])\n",
      "Parameter: prompt_encoder.mask_downscaling.0.bias, Shape: torch.Size([4])\n",
      "Parameter: prompt_encoder.mask_downscaling.1.weight, Shape: torch.Size([4])\n",
      "Parameter: prompt_encoder.mask_downscaling.1.bias, Shape: torch.Size([4])\n",
      "Parameter: prompt_encoder.mask_downscaling.3.weight, Shape: torch.Size([16, 4, 2, 2])\n",
      "Parameter: prompt_encoder.mask_downscaling.3.bias, Shape: torch.Size([16])\n",
      "Parameter: prompt_encoder.mask_downscaling.4.weight, Shape: torch.Size([16])\n",
      "Parameter: prompt_encoder.mask_downscaling.4.bias, Shape: torch.Size([16])\n",
      "Parameter: prompt_encoder.mask_downscaling.6.weight, Shape: torch.Size([256, 16, 1, 1])\n",
      "Parameter: prompt_encoder.mask_downscaling.6.bias, Shape: torch.Size([256])\n",
      "Parameter: prompt_encoder.no_mask_embed.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.q_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.q_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.k_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.v_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.out_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.self_attn.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm1.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm2.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin1.weight, Shape: torch.Size([2048, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin1.bias, Shape: torch.Size([2048])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin2.weight, Shape: torch.Size([256, 2048])\n",
      "Parameter: mask_decoder.transformer.layers.0.mlp.lin2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm3.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm3.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm4.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.norm4.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.q_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.q_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.k_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.v_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.out_proj.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.self_attn.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm1.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm2.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin1.weight, Shape: torch.Size([2048, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin1.bias, Shape: torch.Size([2048])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin2.weight, Shape: torch.Size([256, 2048])\n",
      "Parameter: mask_decoder.transformer.layers.1.mlp.lin2.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm3.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm3.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm4.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.norm4.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.q_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.q_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.k_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.k_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.v_proj.weight, Shape: torch.Size([128, 256])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.v_proj.bias, Shape: torch.Size([128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.out_proj.weight, Shape: torch.Size([256, 128])\n",
      "Parameter: mask_decoder.transformer.final_attn_token_to_image.out_proj.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.norm_final_attn.weight, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.transformer.norm_final_attn.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.iou_token.weight, Shape: torch.Size([1, 256])\n",
      "Parameter: mask_decoder.mask_tokens.weight, Shape: torch.Size([4, 256])\n",
      "Parameter: mask_decoder.output_upscaling.0.weight, Shape: torch.Size([256, 64, 2, 2])\n",
      "Parameter: mask_decoder.output_upscaling.0.bias, Shape: torch.Size([64])\n",
      "Parameter: mask_decoder.output_upscaling.1.weight, Shape: torch.Size([64])\n",
      "Parameter: mask_decoder.output_upscaling.1.bias, Shape: torch.Size([64])\n",
      "Parameter: mask_decoder.output_upscaling.3.weight, Shape: torch.Size([64, 32, 2, 2])\n",
      "Parameter: mask_decoder.output_upscaling.3.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.0.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.1.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.2.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.2.weight, Shape: torch.Size([32, 256])\n",
      "Parameter: mask_decoder.output_hypernetworks_mlps.3.layers.2.bias, Shape: torch.Size([32])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.0.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.0.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.1.weight, Shape: torch.Size([256, 256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.1.bias, Shape: torch.Size([256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.2.weight, Shape: torch.Size([4, 256])\n",
      "Parameter: mask_decoder.iou_prediction_head.layers.2.bias, Shape: torch.Size([4])\n",
      "Parameter: image_encoder.pos_embed, Value: tensor([[[[-0.0921, -0.0891, -0.0830,  ...,  0.9985,  0.9997,  1.0001],\n",
      "          [-0.0749, -0.0724, -0.0670,  ...,  0.9985,  0.9997,  1.0001],\n",
      "          [ 0.0309,  0.0297,  0.0311,  ...,  0.9985,  0.9998,  1.0000],\n",
      "          ...,\n",
      "          [ 0.3893, -0.2003, -0.6958,  ...,  0.9994,  1.0005,  1.0005],\n",
      "          [ 0.5096, -0.1091, -0.6559,  ...,  0.9992,  1.0006,  1.0005],\n",
      "          [ 0.5293, -0.0943, -0.6495,  ...,  0.9992,  1.0006,  1.0005]],\n",
      "\n",
      "         [[-0.0921, -0.0891, -0.0831,  ...,  0.9985,  0.9997,  1.0001],\n",
      "          [-0.0749, -0.0724, -0.0670,  ...,  0.9985,  0.9997,  1.0001],\n",
      "          [ 0.0309,  0.0297,  0.0311,  ...,  0.9985,  0.9998,  1.0000],\n",
      "          ...,\n",
      "          [ 0.3893, -0.2003, -0.6958,  ...,  0.9995,  1.0005,  1.0005],\n",
      "          [ 0.5096, -0.1091, -0.6559,  ...,  0.9993,  1.0006,  1.0005],\n",
      "          [ 0.5293, -0.0942, -0.6495,  ...,  0.9993,  1.0006,  1.0005]],\n",
      "\n",
      "         [[-0.0923, -0.0893, -0.0835,  ...,  0.9986,  0.9998,  1.0000],\n",
      "          [-0.0751, -0.0727, -0.0674,  ...,  0.9986,  0.9998,  1.0000],\n",
      "          [ 0.0308,  0.0295,  0.0308,  ...,  0.9986,  0.9999,  1.0000],\n",
      "          ...,\n",
      "          [ 0.3893, -0.2002, -0.6958,  ...,  0.9998,  1.0005,  1.0005],\n",
      "          [ 0.5096, -0.1090, -0.6559,  ...,  0.9996,  1.0006,  1.0005],\n",
      "          [ 0.5293, -0.0941, -0.6495,  ...,  0.9996,  1.0006,  1.0005]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0900, -0.0843, -0.0842,  ...,  0.9991,  0.9999,  1.0000],\n",
      "          [-0.0729, -0.0678, -0.0682,  ...,  0.9992,  0.9999,  1.0000],\n",
      "          [ 0.0321,  0.0333,  0.0297,  ...,  0.9994,  0.9999,  1.0000],\n",
      "          ...,\n",
      "          [ 0.3881, -0.2001, -0.6931,  ...,  0.9986,  1.0016,  0.9980],\n",
      "          [ 0.5072, -0.1092, -0.6534,  ...,  0.9986,  1.0018,  0.9980],\n",
      "          [ 0.5267, -0.0945, -0.6470,  ...,  0.9986,  1.0018,  0.9981]],\n",
      "\n",
      "         [[-0.0898, -0.0837, -0.0845,  ...,  0.9990,  0.9999,  0.9998],\n",
      "          [-0.0726, -0.0672, -0.0685,  ...,  0.9990,  0.9999,  0.9998],\n",
      "          [ 0.0323,  0.0338,  0.0295,  ...,  0.9993,  0.9998,  0.9998],\n",
      "          ...,\n",
      "          [ 0.3881, -0.2002, -0.6929,  ...,  0.9987,  1.0020,  0.9977],\n",
      "          [ 0.5072, -0.1094, -0.6532,  ...,  0.9987,  1.0021,  0.9978],\n",
      "          [ 0.5268, -0.0946, -0.6468,  ...,  0.9987,  1.0021,  0.9978]],\n",
      "\n",
      "         [[-0.0897, -0.0836, -0.0845,  ...,  0.9989,  0.9999,  0.9998],\n",
      "          [-0.0726, -0.0671, -0.0685,  ...,  0.9990,  0.9998,  0.9998],\n",
      "          [ 0.0323,  0.0339,  0.0295,  ...,  0.9992,  0.9998,  0.9998],\n",
      "          ...,\n",
      "          [ 0.3881, -0.2002, -0.6929,  ...,  0.9987,  1.0020,  0.9977],\n",
      "          [ 0.5072, -0.1094, -0.6531,  ...,  0.9987,  1.0022,  0.9977],\n",
      "          [ 0.5268, -0.0946, -0.6468,  ...,  0.9987,  1.0022,  0.9977]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"/mnt/sda/minkyukim/pth/sam-tutorial_ivdm_adapter/medsam_adapter_0.pth\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "for name, param in checkpoint.items():\n",
    "    print(f\"Parameter: {name}, Shape: {param.shape}\")\n",
    "\n",
    "\n",
    "param_name = list(checkpoint.keys())[0]\n",
    "param_value = checkpoint[param_name]\n",
    "print(f\"Parameter: {param_name}, Value: {param_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing parameters:\n",
      "image_encoder.blocks.10.mlp.lin1.bias: Grad_False\n",
      "image_encoder.blocks.2.attn.rel_pos_h: Grad_False\n",
      "image_encoder.blocks.4.mlp.lin1.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.mlp.lin1.bias: Grad_True\n",
      "image_encoder.blocks.9.mlp.lin1.weight: Grad_False\n",
      "prompt_encoder.point_embeddings.1.weight: Grad_False\n",
      "image_encoder.blocks.6.norm2.weight: Grad_False\n",
      "image_encoder.blocks.10.attn.proj.bias: Grad_False\n",
      "mask_decoder.output_upscaling.0.weight: Grad_True\n",
      "mask_decoder.output_upscaling.3.bias: Grad_True\n",
      "image_encoder.blocks.7.norm1.bias: Grad_False\n",
      "image_encoder.blocks.2.attn.proj.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias: Grad_True\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias: Grad_True\n",
      "image_encoder.blocks.9.attn.proj.bias: Grad_False\n",
      "image_encoder.blocks.4.mlp.lin1.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.mlp.lin1.weight: Grad_True\n",
      "image_encoder.blocks.3.norm2.weight: Grad_False\n",
      "image_encoder.blocks.0.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.6.mlp.lin1.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight: Grad_True\n",
      "mask_decoder.transformer.layers.0.mlp.lin2.weight: Grad_True\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj.bias: Grad_True\n",
      "image_encoder.blocks.8.norm2.weight: Grad_False\n",
      "image_encoder.blocks.0.norm1.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj.weight: Grad_True\n",
      "image_encoder.blocks.6.attn.qkv.bias: Grad_False\n",
      "image_encoder.blocks.2.mlp.lin2.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj.weight: Grad_True\n",
      "image_encoder.blocks.0.attn.rel_pos_w: Grad_False\n",
      "mask_decoder.transformer.layers.0.norm1.weight: Grad_True\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.norm1.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.norm1.weight: Grad_True\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias: Grad_True\n",
      "image_encoder.pos_embed: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight: Grad_True\n",
      "image_encoder.blocks.0.norm1.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.norm4.weight: Grad_True\n",
      "image_encoder.blocks.8.attn.qkv.bias: Grad_False\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj.bias: Grad_True\n",
      "image_encoder.blocks.10.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.7.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.1.norm1.weight: Grad_False\n",
      "image_encoder.blocks.3.attn.qkv.bias: Grad_False\n",
      "prompt_encoder.mask_downscaling.6.bias: Grad_False\n",
      "image_encoder.blocks.9.norm1.weight: Grad_False\n",
      "image_encoder.blocks.10.Adapter.norm.weight: Grad_True\n",
      "image_encoder.blocks.3.norm1.weight: Grad_False\n",
      "image_encoder.blocks.0.Adapter.A: Grad_True\n",
      "image_encoder.blocks.8.norm2.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight: Grad_True\n",
      "image_encoder.blocks.0.Adapter.norm.weight: Grad_True\n",
      "image_encoder.blocks.4.mlp.lin2.bias: Grad_False\n",
      "image_encoder.neck.2.weight: Grad_False\n",
      "image_encoder.blocks.4.Adapter.A: Grad_True\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight: Grad_True\n",
      "image_encoder.blocks.8.attn.rel_pos_h: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias: Grad_True\n",
      "image_encoder.blocks.7.attn.qkv.weight: Grad_False\n",
      "image_encoder.blocks.7.norm1.weight: Grad_False\n",
      "image_encoder.blocks.2.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.8.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.3.attn.qkv.weight: Grad_False\n",
      "image_encoder.blocks.9.Adapter.norm.weight: Grad_True\n",
      "image_encoder.blocks.11.norm1.weight: Grad_False\n",
      "image_encoder.blocks.5.attn.qkv.weight: Grad_False\n",
      "image_encoder.blocks.11.mlp.lin1.bias: Grad_False\n",
      "image_encoder.blocks.5.attn.rel_pos_w: Grad_False\n",
      "prompt_encoder.mask_downscaling.0.weight: Grad_False\n",
      "prompt_encoder.not_a_point_embed.weight: Grad_False\n",
      "image_encoder.blocks.1.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.8.attn.rel_pos_w: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.2.bias: Grad_True\n",
      "image_encoder.blocks.1.mlp.lin1.bias: Grad_False\n",
      "image_encoder.blocks.0.attn.qkv.weight: Grad_False\n",
      "mask_decoder.transformer.layers.1.mlp.lin2.bias: Grad_True\n",
      "image_encoder.blocks.1.Adapter.B: Grad_True\n",
      "image_encoder.blocks.7.Adapter.A: Grad_True\n",
      "image_encoder.blocks.5.mlp.lin1.bias: Grad_False\n",
      "image_encoder.blocks.5.attn.qkv.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj.bias: Grad_True\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias: Grad_True\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias: Grad_True\n",
      "prompt_encoder.mask_downscaling.6.weight: Grad_False\n",
      "prompt_encoder.mask_downscaling.1.weight: Grad_False\n",
      "prompt_encoder.mask_downscaling.4.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.2.weight: Grad_True\n",
      "image_encoder.blocks.8.Adapter.B: Grad_True\n",
      "image_encoder.blocks.8.Adapter.norm.bias: Grad_True\n",
      "image_encoder.blocks.1.Adapter.norm.weight: Grad_True\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj.weight: Grad_True\n",
      "image_encoder.blocks.0.norm2.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.norm2.bias: Grad_True\n",
      "image_encoder.blocks.2.Adapter.B: Grad_True\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj.bias: Grad_True\n",
      "image_encoder.blocks.1.norm1.bias: Grad_False\n",
      "image_encoder.blocks.10.mlp.lin2.bias: Grad_False\n",
      "image_encoder.blocks.3.norm1.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias: Grad_True\n",
      "image_encoder.blocks.3.Adapter.norm.bias: Grad_True\n",
      "image_encoder.blocks.3.Adapter.norm.weight: Grad_True\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight: Grad_True\n",
      "prompt_encoder.mask_downscaling.1.bias: Grad_False\n",
      "image_encoder.blocks.3.Adapter.A: Grad_True\n",
      "mask_decoder.transformer.layers.1.norm4.weight: Grad_True\n",
      "image_encoder.blocks.3.mlp.lin2.weight: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.2.bias: Grad_True\n",
      "image_encoder.blocks.7.Adapter.B: Grad_True\n",
      "image_encoder.blocks.1.mlp.lin2.bias: Grad_False\n",
      "image_encoder.blocks.3.attn.proj.bias: Grad_False\n",
      "prompt_encoder.mask_downscaling.3.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj.weight: Grad_True\n",
      "image_encoder.blocks.6.Adapter.A: Grad_True\n",
      "image_encoder.blocks.6.Adapter.norm.weight: Grad_True\n",
      "mask_decoder.output_upscaling.1.weight: Grad_True\n",
      "image_encoder.blocks.1.Adapter.A: Grad_True\n",
      "image_encoder.blocks.6.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.8.norm1.weight: Grad_False\n",
      "image_encoder.blocks.1.attn.proj.bias: Grad_False\n",
      "image_encoder.blocks.11.attn.proj.weight: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.2.weight: Grad_True\n",
      "image_encoder.patch_embed.proj.weight: Grad_False\n",
      "image_encoder.blocks.7.mlp.lin2.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias: Grad_True\n",
      "image_encoder.blocks.10.norm1.weight: Grad_False\n",
      "image_encoder.blocks.6.Adapter.norm.bias: Grad_True\n",
      "mask_decoder.iou_prediction_head.layers.2.weight: Grad_False\n",
      "image_encoder.blocks.2.norm1.weight: Grad_False\n",
      "image_encoder.blocks.6.Adapter.B: Grad_True\n",
      "image_encoder.blocks.11.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.10.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.6.norm1.weight: Grad_False\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj.bias: Grad_True\n",
      "image_encoder.blocks.1.mlp.lin1.weight: Grad_False\n",
      "mask_decoder.iou_prediction_head.layers.0.bias: Grad_False\n",
      "image_encoder.blocks.4.Adapter.B: Grad_True\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight: Grad_True\n",
      "image_encoder.blocks.7.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.2.attn.qkv.weight: Grad_False\n",
      "image_encoder.blocks.2.mlp.lin1.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.1.bias: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.1.bias: Grad_True\n",
      "image_encoder.blocks.5.norm2.weight: Grad_False\n",
      "image_encoder.blocks.9.attn.qkv.bias: Grad_False\n",
      "image_encoder.blocks.9.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.10.norm1.bias: Grad_False\n",
      "image_encoder.blocks.0.norm2.weight: Grad_False\n",
      "image_encoder.blocks.7.attn.qkv.bias: Grad_False\n",
      "image_encoder.blocks.9.Adapter.A: Grad_True\n",
      "mask_decoder.transformer.norm_final_attn.weight: Grad_True\n",
      "image_encoder.blocks.4.norm2.bias: Grad_False\n",
      "image_encoder.blocks.9.attn.qkv.weight: Grad_False\n",
      "image_encoder.blocks.1.attn.qkv.weight: Grad_False\n",
      "mask_decoder.output_upscaling.1.bias: Grad_True\n",
      "image_encoder.blocks.5.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.3.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.4.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.11.Adapter.norm.weight: Grad_True\n",
      "image_encoder.blocks.1.attn.rel_pos_h: Grad_False\n",
      "image_encoder.blocks.7.norm2.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight: Grad_True\n",
      "image_encoder.blocks.11.Adapter.norm.bias: Grad_True\n",
      "image_encoder.blocks.6.norm2.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.norm3.bias: Grad_True\n",
      "mask_decoder.transformer.norm_final_attn.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.norm3.bias: Grad_True\n",
      "prompt_encoder.mask_downscaling.3.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight: Grad_True\n",
      "image_encoder.blocks.9.mlp.lin2.bias: Grad_False\n",
      "image_encoder.blocks.9.norm2.bias: Grad_False\n",
      "image_encoder.blocks.11.attn.rel_pos_h: Grad_False\n",
      "image_encoder.blocks.10.Adapter.B: Grad_True\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias: Grad_True\n",
      "image_encoder.blocks.9.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.4.norm1.bias: Grad_False\n",
      "mask_decoder.output_upscaling.0.bias: Grad_True\n",
      "image_encoder.neck.1.bias: Grad_False\n",
      "image_encoder.blocks.1.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.8.attn.qkv.weight: Grad_False\n",
      "image_encoder.neck.3.weight: Grad_False\n",
      "image_encoder.blocks.4.Adapter.norm.bias: Grad_True\n",
      "image_encoder.blocks.11.attn.qkv.weight: Grad_False\n",
      "image_encoder.patch_embed.proj.bias: Grad_False\n",
      "image_encoder.blocks.11.norm1.bias: Grad_False\n",
      "image_encoder.blocks.6.attn.proj.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj.bias: Grad_True\n",
      "image_encoder.blocks.5.norm1.bias: Grad_False\n",
      "image_encoder.blocks.9.mlp.lin1.bias: Grad_False\n",
      "image_encoder.blocks.1.norm2.weight: Grad_False\n",
      "prompt_encoder.pe_layer.positional_encoding_gaussian_matrix: Grad_False\n",
      "image_encoder.blocks.4.attn.qkv.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.1.weight: Grad_True\n",
      "image_encoder.blocks.11.mlp.lin2.bias: Grad_False\n",
      "image_encoder.blocks.1.attn.qkv.bias: Grad_False\n",
      "image_encoder.blocks.10.attn.qkv.bias: Grad_False\n",
      "image_encoder.blocks.6.norm1.bias: Grad_False\n",
      "image_encoder.blocks.6.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.2.attn.proj.bias: Grad_False\n",
      "image_encoder.blocks.4.attn.rel_pos_h: Grad_False\n",
      "image_encoder.blocks.5.Adapter.A: Grad_True\n",
      "image_encoder.blocks.2.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.0.attn.qkv.bias: Grad_False\n",
      "image_encoder.blocks.5.mlp.lin2.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0.bias: Grad_True\n",
      "prompt_encoder.point_embeddings.3.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj.bias: Grad_True\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight: Grad_True\n",
      "image_encoder.blocks.3.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.11.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.5.attn.proj.bias: Grad_False\n",
      "image_encoder.blocks.4.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.10.attn.qkv.weight: Grad_False\n",
      "image_encoder.blocks.6.mlp.lin2.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias: Grad_True\n",
      "prompt_encoder.mask_downscaling.0.bias: Grad_False\n",
      "mask_decoder.iou_prediction_head.layers.1.bias: Grad_False\n",
      "image_encoder.blocks.2.Adapter.norm.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj.bias: Grad_True\n",
      "image_encoder.blocks.3.norm2.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0.weight: Grad_True\n",
      "mask_decoder.transformer.layers.0.norm3.weight: Grad_True\n",
      "mask_decoder.mask_tokens.weight: Grad_True\n",
      "mask_decoder.transformer.layers.1.mlp.lin2.weight: Grad_True\n",
      "image_encoder.neck.3.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias: Grad_True\n",
      "image_encoder.blocks.9.norm2.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.norm2.weight: Grad_True\n",
      "image_encoder.blocks.11.norm2.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.2.bias: Grad_True\n",
      "image_encoder.blocks.5.mlp.lin2.weight: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight: Grad_True\n",
      "image_encoder.blocks.7.Adapter.norm.weight: Grad_True\n",
      "image_encoder.blocks.3.mlp.lin2.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.2.weight: Grad_True\n",
      "image_encoder.blocks.2.norm2.bias: Grad_False\n",
      "image_encoder.blocks.6.mlp.lin2.weight: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0.bias: Grad_True\n",
      "image_encoder.blocks.8.mlp.lin2.bias: Grad_False\n",
      "image_encoder.blocks.8.Adapter.norm.weight: Grad_True\n",
      "image_encoder.blocks.10.norm2.bias: Grad_False\n",
      "image_encoder.blocks.11.attn.proj.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight: Grad_True\n",
      "image_encoder.blocks.6.attn.rel_pos_h: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias: Grad_True\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj.bias: Grad_True\n",
      "image_encoder.blocks.8.attn.proj.bias: Grad_False\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj.weight: Grad_True\n",
      "prompt_encoder.mask_downscaling.4.weight: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight: Grad_True\n",
      "image_encoder.blocks.5.norm2.bias: Grad_False\n",
      "image_encoder.blocks.10.Adapter.norm.bias: Grad_True\n",
      "image_encoder.blocks.7.norm2.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj.bias: Grad_True\n",
      "image_encoder.neck.0.weight: Grad_False\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.1.weight: Grad_True\n",
      "image_encoder.blocks.2.norm1.bias: Grad_False\n",
      "mask_decoder.output_upscaling.3.weight: Grad_True\n",
      "image_encoder.blocks.9.Adapter.norm.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj.weight: Grad_True\n",
      "image_encoder.blocks.7.attn.rel_pos_h: Grad_False\n",
      "mask_decoder.transformer.layers.0.norm1.bias: Grad_True\n",
      "image_encoder.blocks.11.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.2.Adapter.norm.weight: Grad_True\n",
      "image_encoder.neck.1.weight: Grad_False\n",
      "image_encoder.blocks.0.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.3.Adapter.B: Grad_True\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.0.bias: Grad_True\n",
      "image_encoder.blocks.9.Adapter.B: Grad_True\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias: Grad_True\n",
      "image_encoder.blocks.0.attn.proj.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj.weight: Grad_True\n",
      "mask_decoder.transformer.layers.1.norm3.weight: Grad_True\n",
      "image_encoder.blocks.5.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.8.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.7.attn.proj.bias: Grad_False\n",
      "image_encoder.blocks.0.Adapter.norm.bias: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.1.weight: Grad_True\n",
      "mask_decoder.transformer.layers.1.norm4.bias: Grad_True\n",
      "image_encoder.blocks.8.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.5.Adapter.B: Grad_True\n",
      "image_encoder.blocks.10.attn.rel_pos_h: Grad_False\n",
      "image_encoder.blocks.10.Adapter.A: Grad_True\n",
      "mask_decoder.iou_token.weight: Grad_True\n",
      "image_encoder.blocks.5.attn.rel_pos_h: Grad_False\n",
      "image_encoder.blocks.5.norm1.weight: Grad_False\n",
      "image_encoder.blocks.8.mlp.lin1.bias: Grad_False\n",
      "mask_decoder.transformer.layers.1.mlp.lin1.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.norm2.weight: Grad_True\n",
      "image_encoder.blocks.9.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.7.attn.proj.weight: Grad_False\n",
      "image_encoder.blocks.0.mlp.lin2.bias: Grad_False\n",
      "image_encoder.blocks.11.Adapter.B: Grad_True\n",
      "image_encoder.blocks.1.Adapter.norm.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj.weight: Grad_True\n",
      "image_encoder.blocks.2.mlp.lin1.weight: Grad_False\n",
      "image_encoder.blocks.5.Adapter.norm.weight: Grad_True\n",
      "image_encoder.blocks.0.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.4.norm1.weight: Grad_False\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj.weight: Grad_True\n",
      "image_encoder.blocks.2.Adapter.A: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0.bias: Grad_True\n",
      "image_encoder.blocks.4.attn.proj.bias: Grad_False\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj.weight: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.1.weight: Grad_True\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj.bias: Grad_True\n",
      "image_encoder.blocks.6.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.2.attn.qkv.bias: Grad_False\n",
      "prompt_encoder.point_embeddings.2.weight: Grad_False\n",
      "image_encoder.blocks.7.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.0.attn.rel_pos_h: Grad_False\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0.weight: Grad_True\n",
      "mask_decoder.iou_prediction_head.layers.0.weight: Grad_False\n",
      "image_encoder.blocks.4.Adapter.norm.weight: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.2.weight: Grad_True\n",
      "mask_decoder.transformer.layers.0.mlp.lin2.bias: Grad_True\n",
      "image_encoder.blocks.2.norm2.weight: Grad_False\n",
      "image_encoder.blocks.7.Adapter.norm.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight: Grad_True\n",
      "image_encoder.blocks.0.Adapter.B: Grad_True\n",
      "mask_decoder.transformer.layers.0.norm2.bias: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.1.bias: Grad_True\n",
      "image_encoder.blocks.10.norm2.weight: Grad_False\n",
      "image_encoder.blocks.9.norm1.bias: Grad_False\n",
      "image_encoder.blocks.8.Adapter.A: Grad_True\n",
      "image_encoder.blocks.9.attn.rel_pos_h: Grad_False\n",
      "mask_decoder.iou_prediction_head.layers.1.weight: Grad_False\n",
      "image_encoder.blocks.11.Adapter.A: Grad_True\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.2.bias: Grad_True\n",
      "image_encoder.blocks.11.norm2.weight: Grad_False\n",
      "image_encoder.blocks.7.mlp.lin1.bias: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.0.weight: Grad_True\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj.weight: Grad_True\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight: Grad_True\n",
      "mask_decoder.iou_prediction_head.layers.2.bias: Grad_False\n",
      "image_encoder.blocks.0.mlp.lin1.bias: Grad_False\n",
      "image_encoder.blocks.8.norm1.bias: Grad_False\n",
      "image_encoder.blocks.3.attn.rel_pos_h: Grad_False\n",
      "image_encoder.blocks.1.mlp.lin2.weight: Grad_False\n",
      "image_encoder.blocks.3.mlp.lin1.bias: Grad_False\n",
      "prompt_encoder.no_mask_embed.weight: Grad_False\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj.weight: Grad_True\n",
      "image_encoder.blocks.10.mlp.lin2.weight: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.1.bias: Grad_True\n",
      "image_encoder.blocks.10.attn.rel_pos_w: Grad_False\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0.weight: Grad_True\n",
      "image_encoder.blocks.5.Adapter.norm.bias: Grad_True\n",
      "mask_decoder.transformer.layers.1.mlp.lin1.weight: Grad_True\n",
      "prompt_encoder.point_embeddings.0.weight: Grad_False\n",
      "image_encoder.blocks.11.attn.qkv.bias: Grad_False\n",
      "image_encoder.blocks.1.norm2.bias: Grad_False\n",
      "image_encoder.blocks.3.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.4.attn.rel_pos_w: Grad_False\n",
      "image_encoder.blocks.4.attn.qkv.weight: Grad_False\n",
      "mask_decoder.transformer.layers.0.norm4.bias: Grad_True\n",
      "image_encoder.blocks.4.norm2.weight: Grad_False\n",
      "image_encoder.blocks.6.attn.qkv.weight: Grad_False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 두 파일의 경로\n",
    "file1_path = \"/mnt/sda/minkyukim/pth/sam-tutorial_ivdm_LoRA/medsam_LoRA_0.pth\"\n",
    "file2_path = \"/mnt/sda/minkyukim/pth/sam-tutorial_ivdm_LoRA/medsam_LoRA_1.pth\"\n",
    "\n",
    "# 두 파일의 state_dict 로드\n",
    "state_dict1 = torch.load(file1_path, map_location=\"cpu\")\n",
    "state_dict2 = torch.load(file2_path, map_location=\"cpu\")\n",
    "\n",
    "# 파라미터 비교\n",
    "print(\"Comparing parameters:\")\n",
    "all_keys = set(state_dict1.keys()).union(set(state_dict2.keys()))\n",
    "for key in all_keys:\n",
    "    if key not in state_dict1:\n",
    "        print(f\"{key}: Only in {file2_path}\")\n",
    "    elif key not in state_dict2:\n",
    "        print(f\"{key}: Only in {file1_path}\")\n",
    "    else:\n",
    "        # 두 텐서를 비교\n",
    "        if torch.equal(state_dict1[key], state_dict2[key]):\n",
    "            print(f\"{key}: Grad_False\")\n",
    "        else:\n",
    "            print(f\"{key}: Grad_True\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
